{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with SAINT (Self-Attention and Intersample Attention Transformer) model\n",
    "\n",
    "Tabular data underpins numerous high-impact applications of machine learning from fraud detection to genomics and healthcare. Classical approaches to solving tabular problems, such as gradient boosting and random forests, are widely used. SAINT, performs attention over both rows and columns, and it includes an enhanced embedding method. SAINT consistently improves performance over previous deep learning methods, and it even outperforms gradient boosting methods, including XGBoost, CatBoost, and LightGBM, on average over a variety of benchmark tasks.\n",
    "\n",
    "<center><img src=\"https://media.arxiv-vanity.com/render-output/6225784/figs/Tabattention_training.png\" alt=\"SAINT pipeline\" width=\"600\"/></center>\n",
    "<center>SAINT model. Source: <a href=\"https://arxiv.org/pdf/2106.01342v1.pdf\">Paper(arxiv),</a> <a href=\"https://www.arxiv-vanity.com/papers/2106.01342/\">image</a>.</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, optuna, torch, pytorch_lightning\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lit_saint import Saint, SaintConfig, SaintDatamodule, SaintTrainer\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from IPython.display import clear_output\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/dataset_df.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "col_types = {\n",
    "    'MONTHS_IN_RESIDENCE':             'float64',\n",
    "    'PERSONAL_MONTHLY_INCOME':         'float64',\n",
    "    'OTHER_INCOMES':                   'float64',\n",
    "    'PERSONAL_ASSETS_VALUE':           'float64',\n",
    "    'MONTHS_IN_THE_JOB':                 'int64',\n",
    "    'AGE':                               'int64'\n",
    "}\n",
    "\n",
    "for col in df.columns:\n",
    "    if col in col_types.keys():\n",
    "        df[col] = df[col].astype(col_types[col])\n",
    "    else:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.20, random_state=42)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.25, random_state=42)\n",
    "df_train[\"split\"] = \"train\"\n",
    "df_val[\"split\"] = \"validation\"\n",
    "df = pd.concat([df_train, df_val])\n",
    "\n",
    "### Configuration object\n",
    "cfg = SaintConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to get and set values to nested attributes of configuration object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsetattr(obj, attr, val):\n",
    "    \"\"\"\n",
    "    Set a value to a nested attribute of an object.\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : Object\n",
    "        object whose nested attribute has to be set.\n",
    "    attr : str\n",
    "        string that contains the nested attribute's name.\n",
    "    val : *\n",
    "        value given to the nested attribute.\n",
    "    \"\"\"\n",
    "    pre, _, post = attr.rpartition('.')\n",
    "    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n",
    "\n",
    "def rgetattr(obj, attr):\n",
    "    \"\"\"\n",
    "    Returns the value of the named nested attribute of an object. \n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : Object\n",
    "        object whose named nested attribute's value is to be returned.\n",
    "    attr : str\n",
    "        string that contains the nested attribute's name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    value of the named nested attribute of the given object.\n",
    "    \"\"\"\n",
    "    def _getattr(obj, attr):\n",
    "        return getattr(obj, attr)\n",
    "    return functools.reduce(_getattr, [obj] + attr.split('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Hyperparameter tunning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = SaintDatamodule(df=df, target=df.columns[28], split_column=\"split\")\n",
    "pretrain_loader_params = {'batch_size': 1000, 'num_workers': 8}\n",
    "train_loader_params = {'batch_size': 10000, 'num_workers': 8}\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna's objective function to be optimized. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trial : Object\n",
    "        Optuna's trial object, defines params grid to feed the SAINT model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    roc_auc_score : float\n",
    "        ROC AUC score of the current trial.\n",
    "    recall_score : float\n",
    "        Recall score of the current trial.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'network.transformer.depth': trial.suggest_int('network.transformer.depth', 2,10,1),\n",
    "        'network.transformer.heads': trial.suggest_int('network.transformer.heads', 1,5,1),\n",
    "        'network.transformer.dropout': trial.suggest_float('network.transformer.dropout', 0.1,0.9),\n",
    "        'network.transformer.dim_head': 2 ** trial.suggest_int('network.transformer.dim_head', 4,7,1),\n",
    "        'pretrain.aug.cutmix.lam': trial.suggest_float('pretrain.aug.cutmix.lam', 0.1,0.9),\n",
    "        'pretrain.aug.mixup.lam': trial.suggest_float('pretrain.aug.mixup.lam', 0.1,0.9),\n",
    "        'pretrain.task.contrastive.nce_temp': trial.suggest_float('pretrain.task.contrastive.nce_temp', 0.1,0.9),\n",
    "        'pretrain.task.contrastive.weight': trial.suggest_float('pretrain.task.contrastive.weight', 0.1,0.9),\n",
    "        'pretrain.task.contrastive.dropout': trial.suggest_float('pretrain.task.contrastive.dropout', 0.1,0.9),\n",
    "        'pretrain.task.denoising.weight_cross_entropy': trial.suggest_float('pretrain.task.denoising.weight_cross_entropy', 0.1,0.9),\n",
    "        'pretrain.task.denoising.weight_mse': trial.suggest_float('pretrain.task.denoising.weight_mse', 0.1,0.9),\n",
    "        'pretrain.task.denoising.dropout': trial.suggest_float('pretrain.task.denoising.dropout', 0.1,0.9),\n",
    "        'pretrain.optimizer.learning_rate': trial.suggest_float('pretrain.optimizer.learning_rate', 1e-5,1e-3, log=True),\n",
    "        'train.optimizer.learning_rate': trial.suggest_float('train.optimizer.learning_rate', 1e-5,1e-3, log=True),\n",
    "        'train.mlpfory_dropout': trial.suggest_float('train.mlpfory_dropout', 0.1,0.9),\n",
    "        'train.internal_dimension_output_layer': trial.suggest_int('train.internal_dimension_output_layer', 10,30,10),\n",
    "        'pretrain.epochs': 90, \n",
    "        'train.epochs': 400\n",
    "    }\n",
    "    \n",
    "    for key in params.keys():\n",
    "        rsetattr(cfg, key, params[key])\n",
    "    print(cfg)\n",
    "    \n",
    "    model = Saint(categories=data_module.categorical_dims, continuous=data_module.numerical_columns,\n",
    "                  config=cfg, dim_target=data_module.dim_target)\n",
    "    pretrainer = Trainer(max_epochs=cfg.pretrain.epochs, accelerator='gpu', devices=-1, log_every_n_steps=30, enable_progress_bar=False)\n",
    "    trainer = Trainer(max_epochs=cfg.train.epochs, accelerator='gpu', devices=-1, log_every_n_steps=15, enable_progress_bar=False)\n",
    "    saint_trainer = SaintTrainer(pretrainer=pretrainer, trainer=trainer, \n",
    "                                pretrain_loader_params=pretrain_loader_params, train_loader_params=train_loader_params)\n",
    "    saint_trainer.fit(model=model, datamodule=data_module, enable_pretraining=True)\n",
    "\n",
    "    prediction = saint_trainer.predict(model=model, datamodule=data_module, df=df_val)\n",
    "    y_pred = prediction[\"prediction\"][:,1]\n",
    "    y_test = df_val[df.columns[28]]\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    return roc_auc_score(y_test, y_pred), recall_score(y_test, y_pred.round())\n",
    "\n",
    "study = optuna.create_study(directions=['maximize', 'maximize'])\n",
    "study.optimize(objective, n_trials=500, timeout=60*60*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of the hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on validation dataset using the best params:\n",
      "ROC AUC: 0.6273642907415584, Recall: 0.01466615206483983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'network.transformer.depth': 7,\n",
       " 'network.transformer.heads': 3,\n",
       " 'network.transformer.dropout': 0.13811672612633155,\n",
       " 'network.transformer.dim_head': 5,\n",
       " 'pretrain.aug.cutmix.lam': 0.7280559655740964,\n",
       " 'pretrain.aug.mixup.lam': 0.535127688615649,\n",
       " 'pretrain.task.contrastive.nce_temp': 0.5680516605800289,\n",
       " 'pretrain.task.contrastive.weight': 0.2691343955809094,\n",
       " 'pretrain.task.contrastive.dropout': 0.518691770224818,\n",
       " 'pretrain.task.denoising.weight_cross_entropy': 0.20017568961839238,\n",
       " 'pretrain.task.denoising.weight_mse': 0.35979232505043923,\n",
       " 'pretrain.task.denoising.dropout': 0.8164470869440895,\n",
       " 'pretrain.optimizer.learning_rate': 3.5047338907672324e-05,\n",
       " 'train.optimizer.learning_rate': 0.0004803097270582274,\n",
       " 'train.mlpfory_dropout': 0.3976845870661899,\n",
       " 'train.internal_dimension_output_layer': 20}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc, recall = study.best_trials[0].values\n",
    "best_params = study.best_trials[0].params\n",
    "print('Metrics on validation dataset using the best params:')\n",
    "print(f'ROC AUC: {roc_auc}, Recall: {recall}')\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_module = SaintDatamodule(df=df, target=df.columns[28], split_column=\"split\")\n",
    "pretrain_loader_params = {'batch_size': 2500, 'num_workers': 8}\n",
    "train_loader_params = {'batch_size': 10000, 'num_workers': 8}\n",
    "params = {\n",
    "    'pretrain.epochs': 90,\n",
    "    'train.epochs': 400\n",
    "}\n",
    "for key in params.keys():\n",
    "    rsetattr(cfg, key, params[key])\n",
    "for key in best_params.keys():\n",
    "    rsetattr(cfg, key, params[key])\n",
    "\n",
    "model = Saint(categories=data_module.categorical_dims, continuous=data_module.numerical_columns,\n",
    "              config=cfg, dim_target=data_module.dim_target)\n",
    "pretrainer = Trainer(max_epochs=cfg.pretrain.epochs, accelerator='gpu', devices=-1, log_every_n_steps=10, enable_progress_bar=False)\n",
    "trainer = Trainer(max_epochs=cfg.train.epochs, accelerator='gpu', devices=-1, log_every_n_steps=1, enable_progress_bar=False)\n",
    "saint_trainer = SaintTrainer(pretrainer=pretrainer, trainer=trainer, \n",
    "                             pretrain_loader_params=pretrain_loader_params, train_loader_params=train_loader_params)\n",
    "saint_trainer.fit(model=model, datamodule=data_module, enable_pretraining=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7e0c499b2d48f8bdddfaf7e4c1ed9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 3it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC: 0.6204, Recall: 0.0000\n"
     ]
    }
   ],
   "source": [
    "prediction = saint_trainer.predict(model=model, datamodule=data_module, df=df_val)\n",
    "y_pred = prediction[\"prediction\"][:,1]\n",
    "y_test = df_val[df.columns[28]]\n",
    "print(f'ROC_AUC: {roc_auc_score(y_test, y_pred):.4f}, Recall: {recall_score(y_test, y_pred.round()):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "7167a699fb2e3f3e89ad5d52ada2ded4a980cd4f2ad3a543c050d80dc41b020b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
