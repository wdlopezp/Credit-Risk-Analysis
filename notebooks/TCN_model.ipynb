{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb13315",
   "metadata": {},
   "source": [
    "# Training TCN (Temporal Convolutional Network) model\n",
    "\n",
    "In addition to convolution and dropout, TCNs utilize dilation to enable larger receptive field sizes, introducing a fixed step size between every two adjacent convolutional filter taps. TCNs also employ residual connections so that each convolutional layer learns an identity mapping rather than the entire transformation, which helps stabilize deeper and wider models. Within each residual TCN block, layer normalization is applied to the output of the convolutional layer and dropout is added for regularization.\n",
    "\n",
    "<center><img src=\"https://i.ibb.co/prTGdnT/TCN-arquitecture.png\" alt=\"TCN-arquitecture\" alt=\"TCN arquitecture\" width=\"400\"/></center>\n",
    "<center>TCN model. Source: <a href=\"https://arxiv.org/pdf/2012.15330.pdf\">Paper(arxiv)</a>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a696c",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aebfb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 21:07:17.568729: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow_addons.layers import WeightNormalization as weight_norm\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import optuna "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4df85",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c39f9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/dataset.pkl', 'rb') as f:\n",
    "    features, labels = pickle.load(f)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1564f064",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041c3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN_block(keras.Model):\n",
    "    def __init__(self, filters, kernel_size, dilation, dropout=0.2):\n",
    "        super(TCN_block, self).__init__()\n",
    "        self.conv = weight_norm(layers.Conv1D(filters, kernel_size, padding=\"causal\", activation='relu',\n",
    "                                              dilation_rate=dilation))\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv(inputs)\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "        return x + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec8b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN_Model(keras.Model):\n",
    "    def __init__(self, n_features, dense_layer_size, kernel_size, dilation, dropout):\n",
    "        super(TCN_Model, self).__init__()\n",
    "        self.tcn1 = TCN_block(n_features, kernel_size, dilation, dropout=dropout)\n",
    "        self.tcn2 = TCN_block(n_features, kernel_size, dilation, dropout=dropout)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(dense_layer_size, activation='relu')\n",
    "        self.dense2 = layers.Dense(dense_layer_size//min(10,dense_layer_size), activation='relu')\n",
    "        self.dense3 = layers.Dense(dense_layer_size//min(50,dense_layer_size), activation='relu')\n",
    "        self.dense4 = layers.Dense(dense_layer_size//dense_layer_size, activation='sigmoid')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.tcn1(x)\n",
    "        x = self.tcn2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return self.dense4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5071b3e8",
   "metadata": {},
   "source": [
    "### Define the Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "030ff6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessor proc\n",
    "def outlier_handler(df):\n",
    "    q1 = df.quantile(0.25)\n",
    "    q3 = df.quantile(0.75)\n",
    "    IQR = q3-q1\n",
    "    lwr_bound = q1-(1.5*IQR)\n",
    "    upr_bound = q3+(1.5*IQR)\n",
    "    df = np.where(df > upr_bound, df.median(), np.where(df < lwr_bound, df.median(), df))\n",
    "    return df\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('Outlier_handler', FunctionTransformer(outlier_handler)),\n",
    "    ('Imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('Imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('Binary_encoder', OneHotEncoder(sparse=False, drop='if_binary', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numeric_features = X_train.select_dtypes(['int64', 'float64']).columns\n",
    "cat_cols = X_train.select_dtypes('category').columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "proc = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2e938",
   "metadata": {},
   "source": [
    "### Obteining processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d5cc2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toldex/miniconda3/envs/torch/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:188: UserWarning: Found unknown categories in columns [5] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def process(X, y, proc_fit_transform):\n",
    "    X = proc_fit_transform(X)\n",
    "    X = X.reshape([X.shape[0], 1, X.shape[1]])\n",
    "    y = y.to_numpy().reshape([X.shape[0], 1])\n",
    "    return X, y\n",
    "\n",
    "X_train_proc, y_train_proc= process(X_train, y_train, proc.fit_transform)\n",
    "X_test_proc, y_test_proc= process(X_test, y_test, proc.transform)\n",
    "X_val_proc, y_val_proc= process(X_val, y_val, proc.transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c20e71",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c656e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    dense_layer_size = trial.suggest_int('dense_layer_size', 50, 10000, 50)\n",
    "    kernel_size = trial.suggest_int('kernel_size', 1, 10)\n",
    "    dilation = trial.suggest_int('dilation', 1, 10)\n",
    "    dropout = trial.suggest_float('dropout', 0., 1.)\n",
    "\n",
    "    model = TCN_Model(X_train_proc.shape[2], dense_layer_size, kernel_size, dilation, dropout)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                loss=keras.losses.BinaryCrossentropy(),\n",
    "                metrics=[keras.metrics.AUC(),\n",
    "                        keras.metrics.Recall()])\n",
    "    model.fit(X_train_proc, y_train_proc, batch_size=10000, epochs=epochs, verbose=0)                       \n",
    "    y_pred = model.predict(X_val_proc)\n",
    "\n",
    "    return recall_score(y_val_proc, y_pred.round())\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=300, timeout=60*60*12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342101f7",
   "metadata": {},
   "source": [
    "### Results of the hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d357214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense_layer_size': 2850,\n",
       " 'kernel_size': 9,\n",
       " 'dilation': 7,\n",
       " 'dropout': 0.06119379214886582}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be6ffc3",
   "metadata": {},
   "source": [
    "### Trainning the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff5c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer_size = 2850\n",
    "kernel_size = 9\n",
    "dilation = 7\n",
    "dropout = 0.06\n",
    "\n",
    "model = TCN_Model(X_train_proc.shape[2], dense_layer_size, kernel_size, dilation, dropout)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[keras.metrics.AUC(),\n",
    "                       keras.metrics.Recall()])\n",
    "hist = model.fit(X_train_proc, y_train_proc, batch_size=10000, epochs=100, \n",
    "          validation_data=(X_val_proc, y_val_proc), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92588464",
   "metadata": {},
   "source": [
    "### Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b4c9ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC: 0.5425, Recall: 0.2087\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_proc)\n",
    "print(f'ROC_AUC: {roc_auc_score(y_test, y_pred):.4f}, Recall: {recall_score(y_test, y_pred.round()):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "7167a699fb2e3f3e89ad5d52ada2ded4a980cd4f2ad3a543c050d80dc41b020b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
